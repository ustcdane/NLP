## EM 算法用途
EM算法是含有隐变量的概率模型参数估计方法。它是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭代算法。

## EM算法求解过程
 含有隐变量的概率模型的数据表示为P(Y,Z|θ),这里,Y是观察变量的数据，Z是隐变量的数据，θ是模型参数。EM算法通过迭代求解观测数据的对数似然函数
 L(θ) = logP(Y|θ)的极大化，实现极大似然估计，每次迭代包含两步：
 1. E步，求期望，即求P(Y,Z|θ)关于Z的后验概率P(Z|Y,θ^(i))的期望：
   Q(θ,θ^(i)) = sum_z{P(Z|Y,θ^(i))logP(Y,Z|θ)}
   这个函数称为Q函数，其中θ^(i)是参数的现估计值。
 2. M步，求极大，即极大化Q函数得到参数的新估计值：
	θ^(i) = arg max Q(θ,θ^(i)) 

在构建EM算法时，重要的是定义Q函数，每次迭代中，EM算法通过极大化Q函数来增大对数似然函数L(θ).

## EM 算法能否全局最优？

EM算法在每次迭代后均提高观测数据的似然函数值，即
		P(Y|θ^(i+1)) > P(Y|θ^(i))
一般条件下EM算法是收敛的，但不能保证收敛到全局最优。
   
   
   
## EM算法的应用
有以下经典应用
 1. 混合高斯模型
 2. HMM（隐马尔科夫模型）
 3. PLSA(基于概率统计的隐性语义分析)
 ...